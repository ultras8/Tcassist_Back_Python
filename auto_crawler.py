import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pdf_ai_parser import parse_pdf_to_criteria # ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏°‡∏≤‡πÉ‡∏ä‡πâ
from sync_manager import sync_data

def crawl_and_process(target_url):
    print(f"üåê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö: {target_url}")
    
    try:
        # 1. ‡πÑ‡∏õ‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ PDF ‡πÑ‡∏´‡∏°
        response = requests.get(target_url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏Å‡πå (<a>) ‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ .pdf
        pdf_links = []
        for a in soup.find_all('a', href=True):
            if a['href'].endswith('.pdf'):
                # ‡πÅ‡∏õ‡∏•‡∏á URL ‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô URL ‡πÄ‡∏ï‡πá‡∏° (‡πÄ‡∏ä‡πà‡∏ô /doc.pdf -> https://uni.ac.th/doc.pdf)
                full_url = urljoin(target_url, a['href'])
                pdf_links.append(full_url)
        
        if not pdf_links:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞")
            return

        # 2. ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏∏‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏Å‡πå‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ô‡∏∞)
        target_pdf = pdf_links[0]
        file_name = "downloaded_temp.pdf"
        
        print(f"üì• ‡πÄ‡∏à‡∏≠ PDF ‡πÅ‡∏•‡πâ‡∏ß! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î: {target_pdf}")
        pdf_data = requests.get(target_pdf).content
        with open(file_name, 'wb') as f:
            f.write(pdf_data)

        # 3. ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ AI (‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ)
        print("ü§ñ ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡πà‡∏≠...")
        extracted_data = parse_pdf_to_criteria(file_name)

        # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Database
        if extracted_data:
            sync_data(extracted_data)
            print("üöÄ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢! ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö")

        # 5. (Optional) ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à
        # os.remove(file_name)

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏ó‡πà‡∏≠‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")

if __name__ == "__main__":
    # ‡∏•‡∏≠‡∏á‡πÉ‡∏™‡πà URL ‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏•‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á‡∏™‡∏ô‡πÉ‡∏à
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á ‡∏°.‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏°‡∏´‡∏≤‡∏•‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏¥‡πâ‡∏á‡∏Å‡πå PDF ‡∏ß‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
    test_url = "https://admission.ku.ac.th/‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£" 
    crawl_and_process(test_url)